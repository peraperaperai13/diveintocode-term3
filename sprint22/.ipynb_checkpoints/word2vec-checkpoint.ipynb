{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vecとは？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前置き"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語をベルトルに変換する手法は主に2つあります。  \n",
    "\n",
    "- **カウントベース**  \n",
    "- **推論ベース**  \n",
    "\n",
    "このうち、**推論ベース**と呼ばれる手法が**word2vec**です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ポイント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語をニューラルネットワークに入力するには、**単語をベクトルに変換**する必要があります。これがミソです。  \n",
    "ここでのベクトルはone-hot表現の行列のことを言います。  \n",
    "なぜ単語をベクトルに変換することがミソかと言うと、**演算可能になるから**です。  \n",
    "例えば、りんごが[1, 1]、みかんが[1, 2]のベクトルで表せれるとすると、りんご[1, 1]に重み[0, 1]を足し算すれば、  \n",
    "みかん[1, 2]を得ることができます。この例は極端かもしれませんが、  \n",
    "**単語をベクトルに変換し、演算可能にする**ことで推論を可能にしているのです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vecのモデルは？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-BOW（continuous bag-of-words）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "簡単に言うと、ターゲット（予測したい単語）を周囲のコンテキストから予測する手法のことです。  \n",
    "例えば、**You ○○○ goodbye, I say hello.**というテキストデータが与えられた時、  \n",
    "○○○の周囲の単語（You, goodbye）から○○○に入る単語を予測させることがC-BOWの手法になります。  \n",
    "テキストデータをニューラルネットワークの入力データとして使用し、出力として当てはまる単語の確率を出します。  \n",
    "よって、ニューラルネットワークに入力するデータはターゲットの周囲にあるコンテキストになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "簡単に言うと、ある単語から周囲のコンテキスト（ターゲットとなる複数の単語）を予測する手法のことです。  \n",
    "例えば、○○○ say ○○○, I say hello.というテキストデータが与えられた時、  \n",
    "sayから周囲の単語（○○○、○○○）に入る単語を予測させることがskip-gramの手法です。  \n",
    "よって、ニューラルネットワークに入力するデータはターゲットの間にある単語になります。  \n",
    "前述したC-BOWと全く逆のアプローチです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C-BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算グラフ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スクラッチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    words = text.replace('.', ' .').split(' ')\n",
    "    \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "    \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 1, 5, 6])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(corpus):\n",
    "    return np.identity(len(np.unique(corpus)))[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = one_hot(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_contexts_target(corpus, window_size=1):\n",
    "    contexts = []\n",
    "    target = corpus[window_size:-window_size]\n",
    "    \n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        context = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            context.append(corpus[idx + t])\n",
    "        contexts.append(context)\n",
    "\n",
    "    return np.array(contexts), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts, target = make_contexts_target(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(contexts.shape)\n",
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(target.shape)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2, 7)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_loss(output):\n",
    "    exp_out = np.exp(output + 1e-08)\n",
    "    exp_out_sum = np.sum(np.exp(output + 1e-08))\n",
    "    softmax =  exp_out / exp_out_sum\n",
    "    return -np.sum(np.log(softmax) * target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(contexts, target):\n",
    "    hidden_size = contexts.shape[1] + 1\n",
    "    w_in = np.random.randn(contexts.shape[2], hidden_size)\n",
    "    w_out = np.random.randn(hidden_size, contexts.shape[2])\n",
    "\n",
    "    cross_entropy = []\n",
    "    for context in contexts:\n",
    "        context_dot = []\n",
    "        for word in context:\n",
    "            word = word.reshape(1, word.shape[0])\n",
    "            h = np.dot(word, w_in)\n",
    "            context_dot.append(h)\n",
    "        h_sum = np.sum(np.array(context_dot), axis=0)\n",
    "        h_out = np.dot(h_sum, w_out)\n",
    "        \n",
    "        loss = forward_loss(h_out)\n",
    "        cross_entropy.append(loss)\n",
    "        \n",
    "    return np.sum(np.array(cross_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward = forward(contexts, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "922951.3155415655"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dw = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dw\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmaxの出力\n",
    "        self.t = None  # 教師ラベル\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 教師ラベルがone-hotベクトルの場合、正解のインデックスに変換\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vecの改良"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力データはone-hotをかけた行列になるため、単語数が多くなるにつれ、計算量が異常に増えます。  \n",
    "隠れ層の計算は対象の単語のベクトルを抽出することでしたが、これまで実装したやり方だと、  \n",
    "その他の関係ない単語のベクトルを計算対象と入れてしまっているため、計算コストが非常に大きいです。  \n",
    "そのため、Embeddingを用いて、巨大な行列の計算対象となる部分のみ抽出し、演算を行うように改良します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまでは多値分類としてSoftmax関数を使ってcross entropyを計算していましたが、  \n",
    "それを二値分類として扱い、Sigmoid関数を用いて計算コストを削減する方法をとります。  \n",
    "また、これまでは計算対象でないベクトルまでSoftmax関数にかけていましたが、  \n",
    "Embeddingを用いて、計算対象となるベクトルのみをSigmoid関数にかけていきます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし、これだけでは正しい単語しか学習できません。  \n",
    "つまり、具体的にどんなベクトルが正しいもので、どんなベクトルが間違っているかまでは学習ができていないということです。  \n",
    "そのため、間違っているベクトルも学習をさせるというのがNegative samplingの考え方です。  \n",
    "しかし、間違っているベクトルは無数にあるので、全てを学習させるのは現実的でないので、  \n",
    "少数の間違いベクトルを学習させ、正しいベクトルと間違っているベクトルの両方を学習させ、その損失を得ることが目標となります。  \n",
    "少数の間違いベクトルをサンプリングすることをNegative samplingと言います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative samplingはランダムでサンプリングするのではなく、コーパスの統計データに基づいてサンプリングを行います。  \n",
    "つまり、コーパスの中でよく使われる単語はサンプリングされやすく、あまり使われない単語はサンプリングされにくくするということです。  \n",
    "具体的には、コーパスの中で出てくる単語の出現回数をカウントし、確率分布を作成して、そこからサンプリングします。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramSampler:\n",
    "    def __init__(self, corpus, power, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.word_p = None\n",
    "        self.vocab_size = None\n",
    "\n",
    "        count_word = {}\n",
    "        for i in range(len(corpus)):\n",
    "            if str(corpus[i]) not in count_word:\n",
    "                count_word[str(corpus[i])] = 1\n",
    "            else:\n",
    "                count_word[str(corpus[i])] += 1\n",
    "\n",
    "        self.p_words = np.array(list(count_word.values()))\n",
    "        self.p_words = np.power(self.p_words, power)\n",
    "        self.p_words /= np.sum(self.p_words)\n",
    "        self.vocab_size = len(self.p_words)\n",
    "        \n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "        \n",
    "#         negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "        \n",
    "        return np.random.choice(self.vocab_size, size=(batch_size, self.sample_size), replace=False, p=self.p_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_word = {}\n",
    "for unique_c in np.unique(corpus):\n",
    "    count_word[unique_c] = 0\n",
    "\n",
    "for c in corpus:\n",
    "    count_word[c] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[1. 0. 0. 0. 0. 0. 0.]': 1,\n",
       " '[0. 1. 0. 0. 0. 0. 0.]': 2,\n",
       " '[0. 0. 1. 0. 0. 0. 0.]': 1,\n",
       " '[0. 0. 0. 1. 0. 0. 0.]': 1,\n",
       " '[0. 0. 0. 0. 1. 0. 0.]': 1,\n",
       " '[0. 0. 0. 0. 0. 1. 0.]': 1,\n",
       " '[0. 0. 0. 0. 0. 0. 1.]': 1}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_word = {}\n",
    "for i in range(len(corpus)):\n",
    "    if str(corpus[i]) not in count_word:\n",
    "        count_word[str(corpus[i])] = 1\n",
    "    else:\n",
    "        count_word[str(corpus[i])] += 1\n",
    "\n",
    "count_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 6, 4, 0, 3])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power = 0.75\n",
    "sample_size = 5\n",
    "\n",
    "count_word = {}\n",
    "for i in range(len(corpus)):\n",
    "    if str(corpus[i]) not in count_word:\n",
    "        count_word[str(corpus[i])] = 1\n",
    "    else:\n",
    "        count_word[str(corpus[i])] += 1\n",
    "        \n",
    "p_words = np.array(list(count_word.values()))\n",
    "p_words = np.power(p_words, power)\n",
    "p_words /= np.sum(p_words)\n",
    "vocab_size = len(p_words)\n",
    "\n",
    "np.random.choice(vocab_size, size=sample_size, replace=False, p=p_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[[1, 6, 4, 0, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# サーベイ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 埋め込み学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "197px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
