{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNとは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "リカレントニューラルネットワーク（RNN）とは何なのかを簡潔に説明してください。\n",
    "\n",
    "説明に含める事項\n",
    "\n",
    "- 再帰的とはどういうことか  \n",
    "- これまでのニューラルネットワークとは何が違うのか  \n",
    "- どういったときに使われるのか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNはRecurrent Neural Networkの略称であり、**Recurrent**は**何度も繰り返し起こること**を意味します。  \n",
    "これは日本語で**再帰する**や**循環する**という風に訳されます。この「再帰する」や「循環する」ということは、  \n",
    "ある地点をスタートしたものが、時間を経て再びその地点に戻ってくることを言います。つまり、**再び戻ってくる**、**ループする**ということです。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNでは、循環させることで時系列データを扱うことを可能にしています。  \n",
    "今までのニューラルネットワークでは時系列データを扱うことはできませんでした。  \n",
    "RNNは循環経路を持っているため、過去の情報を記憶しながら最新のデータへと更新することができるため、時系列データを扱うことができます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えば、(x1, x2, ・・・・xt, ・・・・・)というデータが入力されたとします。  \n",
    "すると、計算は「**$x1 * W = h1$**」、「**$x2 * W = h2$**」、・・・・・という風に行なっていきます。  \n",
    "これを循環させるには**$h1$**と**$x2$**を足し合わせる、というように、  \n",
    "前の計算結果を今回の計算対象となる$x$に足し合わせていき、活性化関数に通し、  \n",
    "その出力を次の計算対象となる$x$に足し合わせて、活性化関数に通す・・・ということを繰り返します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体的に、**You say goodbye and I say hello.**というテキストを入力する場合、  \n",
    "**You, say, goodbye, and, I, say, hello, .**という単語に分けて演算を行います。  \n",
    "最初にYouを演算し、その結果をsayに足し合わせて演算し、その結果をgoodbyeに足し合わせて演算し、・・・・・ということを繰り返します。  \n",
    "これがRNNの仕組みです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サンプルを回してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset\n",
      "padding\n",
      "build model\n",
      "train\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.5107 - acc: 0.7445 - val_loss: 0.4452 - val_acc: 0.7948\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.3106 - acc: 0.8707 - val_loss: 0.5409 - val_acc: 0.7333\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 38s 2ms/step - loss: 0.1481 - acc: 0.9464 - val_loss: 0.6453 - val_acc: 0.7711\n",
      "25000/25000 [==============================] - 7s 261us/step\n",
      "Test score: 0.6453347884750367\n",
      "Test accuracy: 0.77108\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 40\n",
    "batch_size = 32\n",
    "\n",
    "print('load dataset')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "print('padding')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print('build model')\n",
    "inp = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(max_features, 128)(inp)\n",
    "simple_rnn_out = SimpleRNN(32)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(simple_rnn_out)\n",
    "model = Model(inputs=inp, outputs=predictions)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('train')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('./deep-learning-from-scratch-2')\n",
    "from common.util import preprocess, convert_one_hot, create_contexts_target\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from common.layers import Embedding, SoftmaxWithLoss\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from ch04.cbow import CBOW\n",
    "#from ch04.train import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say goodbye and I say hello.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you [-1.46742957e-02  3.29142692e-03 -2.59777345e-02  3.77275865e-03\n",
      " -9.94054321e-03 -1.37563059e-02 -1.05478093e-02  5.14214067e-03\n",
      "  3.79676395e-03 -2.96720560e-03 -9.59710311e-03  4.15629754e-03\n",
      " -7.94642698e-03 -2.18135621e-02 -1.06690265e-02  6.45278161e-03\n",
      "  1.38266394e-02  9.02400725e-03  8.63070413e-03 -6.96519716e-03\n",
      " -2.77075288e-03  2.08560843e-02 -1.35048758e-02  2.24404037e-03\n",
      "  1.22225303e-02 -4.60114609e-03 -5.43160504e-03 -5.36746206e-03\n",
      "  2.38862191e-03  1.66661013e-02 -1.26177501e-02 -7.78823596e-05\n",
      "  1.23316366e-02 -1.17378472e-03  5.20643406e-03  1.37860458e-02\n",
      " -1.68333687e-02  4.49968548e-03 -9.21047200e-03 -2.77929055e-03\n",
      " -2.74790381e-03 -1.69383525e-03  4.70042834e-03 -5.68332907e-04\n",
      " -1.00054704e-02  9.89604462e-03  1.91167500e-02 -1.37223620e-02\n",
      " -1.58599634e-02 -7.88927730e-03 -3.07929958e-03 -5.12957992e-03\n",
      " -5.12091676e-03 -1.47167286e-02  1.39234085e-02  9.07599274e-03\n",
      "  5.07938722e-03 -1.23475362e-02 -8.86251684e-03 -2.06572153e-02\n",
      "  2.26035975e-02  2.93164817e-03 -2.57778727e-03  2.32553445e-02\n",
      "  1.30126355e-02 -9.38352023e-04 -1.10235261e-02  8.33748654e-03\n",
      "  2.49249511e-03 -3.04069696e-03 -9.75491200e-03  5.99211501e-03\n",
      " -1.08446488e-02  2.81294691e-03  7.42652593e-03 -4.22215182e-03\n",
      "  8.87069479e-03 -9.32985160e-04  1.12069829e-03 -3.24529858e-04\n",
      "  8.01039301e-03  1.21748000e-02  9.46256053e-03 -1.21512413e-02\n",
      "  6.39521005e-03  1.42093534e-02 -8.98261089e-03 -1.13798305e-02\n",
      "  2.93205027e-03  1.43612549e-02  5.33847790e-03 -6.45819632e-03\n",
      " -1.00644482e-02 -3.03079636e-04 -5.35950577e-03 -1.46148773e-03\n",
      " -1.39990486e-02 -9.13298689e-04  1.89459405e-03  1.04661174e-02]\n",
      "say [ 1.13725830e-02 -1.11422194e-02 -8.78772233e-03  2.86342949e-03\n",
      "  8.22938699e-03 -8.75644572e-03  7.92615116e-03  9.37154703e-03\n",
      "  3.41550703e-03  1.11460174e-02  7.86453392e-03  2.07820293e-02\n",
      " -9.10648424e-03 -1.07283555e-02  1.51230302e-03  1.21999066e-02\n",
      "  9.76008363e-03 -9.41544864e-03  4.70747000e-05 -7.65012437e-03\n",
      " -9.11267358e-04 -7.61746336e-03 -5.22042997e-03 -9.68575384e-03\n",
      " -7.48435641e-03 -9.53540730e-04 -6.89908350e-03  3.13418801e-03\n",
      " -6.10463507e-03 -1.07411202e-02  5.99935139e-03  4.08961118e-04\n",
      "  6.74421992e-03 -1.39819905e-02 -4.61188471e-03  5.15914289e-03\n",
      "  3.11949896e-03  6.69255061e-03  2.88396585e-03 -8.26598052e-03\n",
      " -5.25704585e-04 -1.41893199e-03  3.09314812e-03  5.61065535e-05\n",
      " -4.84524854e-03  4.66320058e-03 -3.08204768e-03  6.04777969e-03\n",
      "  4.10183752e-03  1.16172051e-02  1.83746184e-03  4.53733280e-03\n",
      " -1.67035486e-03 -3.95605294e-03 -2.78237760e-02 -6.49597822e-03\n",
      "  3.06389504e-03  2.26519071e-02 -3.54303163e-03  2.35013869e-02\n",
      " -1.80646051e-02  4.27963771e-03 -2.16249395e-02  2.10099830e-03\n",
      " -1.14555918e-02  1.05191942e-03 -1.58536769e-02  7.33661465e-03\n",
      " -7.05179153e-03  1.06215365e-02 -1.06990337e-02 -1.53042311e-02\n",
      " -9.72734008e-04 -5.57352509e-03  1.70089526e-03  5.86704072e-03\n",
      " -6.71141688e-03  5.05490880e-03 -1.04301870e-02 -6.79352740e-03\n",
      "  6.14685472e-03  5.73360408e-03 -6.96004741e-03  1.27210107e-03\n",
      "  1.90836806e-02 -1.66252919e-03  2.01519323e-03  1.15973419e-02\n",
      "  5.86475898e-03 -9.02139861e-03 -2.16174126e-02  4.34493925e-03\n",
      "  1.54662365e-02  2.17770860e-02  5.10328216e-03 -1.77879091e-02\n",
      "  2.77746543e-02 -7.87429512e-04 -2.12212633e-02 -4.90879547e-03]\n",
      "goodbye [ 2.37581274e-03  1.96048943e-03  3.14669148e-03 -2.07012035e-02\n",
      "  1.02750035e-02  2.17364468e-02 -1.29088596e-03 -1.70780923e-02\n",
      "  8.45054630e-03  2.33480521e-03  4.22656815e-03 -1.91287659e-02\n",
      " -5.34245279e-03 -1.55858407e-02 -6.30903989e-04 -9.45659820e-03\n",
      "  5.08771744e-03  1.45871309e-03 -1.42649505e-02  1.90989790e-03\n",
      " -4.94831055e-03 -7.02232355e-03  9.15812980e-03 -2.48772521e-02\n",
      "  2.97782617e-03 -9.48098395e-03 -6.94181910e-03 -5.12280688e-03\n",
      "  8.41240119e-03 -5.95479598e-03  9.16434173e-03 -6.37208577e-03\n",
      "  1.58574879e-02  2.60430661e-05 -1.08186423e-03 -1.39868921e-02\n",
      " -6.77082129e-03 -1.70624160e-04 -2.21509002e-02  5.75961452e-03\n",
      " -3.77538917e-03 -1.35465777e-02 -4.21829429e-03  2.67259255e-02\n",
      " -4.73514153e-03  1.94027077e-03  1.80242525e-03 -3.33733507e-04\n",
      "  2.26217182e-03 -1.49961580e-02  5.93342306e-03  7.28250435e-03\n",
      " -1.95579641e-02  2.91776705e-05  6.37300964e-03 -1.53186619e-02\n",
      " -8.26216582e-03  5.19086933e-03 -1.57277603e-02 -9.16183367e-03\n",
      " -1.26075582e-03  5.73364785e-03  5.95490215e-03 -3.41507606e-03\n",
      "  2.40212888e-03 -1.90954469e-03 -1.02432938e-02 -8.17251578e-03\n",
      " -1.57508784e-05 -6.28125668e-03 -2.56061694e-03  7.27963075e-03\n",
      "  5.00521529e-03  3.08515294e-03 -6.63954439e-03 -8.10959656e-03\n",
      " -6.38462929e-03  2.50532720e-02  1.68000516e-02 -1.22348918e-02\n",
      "  5.14728110e-03  8.44555721e-03 -7.95198884e-03  1.33623676e-02\n",
      " -2.74624932e-03  1.68977759e-03 -1.04263173e-02  2.31444859e-03\n",
      " -5.45977568e-03  4.31023352e-03 -2.12614648e-02 -1.55531720e-03\n",
      "  1.16193220e-02 -3.42714507e-03  6.83128834e-03  7.16682198e-03\n",
      " -9.66351572e-03  3.64361377e-03 -8.60448740e-03  2.29563098e-03]\n",
      "and [ 1.69239880e-03 -3.33891623e-03  1.09663308e-02  8.30632914e-03\n",
      "  9.58376750e-03  1.21832720e-03  1.34185050e-02  9.79891606e-03\n",
      "  4.92621493e-03 -9.18106700e-04  1.42945303e-02  3.14599165e-04\n",
      "  1.30269080e-02 -4.55693854e-03  7.87003804e-03  5.46821067e-03\n",
      " -1.12967556e-02  1.15705142e-02 -8.74619372e-03  8.05174839e-03\n",
      "  2.81926990e-03  1.54893575e-02  8.04550480e-03  1.93909137e-03\n",
      " -9.46044270e-03  8.01201630e-03  7.32974708e-03 -1.17115944e-03\n",
      " -1.04859294e-02 -1.53371915e-02 -1.43338018e-03  1.41404066e-02\n",
      "  6.69038482e-03 -4.19096509e-03  1.82768889e-02 -2.24623363e-03\n",
      " -6.09004498e-03  2.49180268e-03  3.45919118e-03 -1.43821621e-02\n",
      "  2.31050956e-03  2.33547520e-02  8.59528966e-03  1.11607974e-03\n",
      " -7.91782420e-03 -1.37624927e-02 -5.46883373e-03  3.83023988e-03\n",
      " -1.52085889e-02 -1.33550102e-02  3.31810536e-03 -5.29712066e-03\n",
      " -6.17743190e-03 -1.44761289e-02  3.08176689e-03 -1.05854892e-03\n",
      "  1.05394246e-02 -5.39825903e-03 -1.76434983e-02 -9.93701816e-03\n",
      "  5.36570151e-04 -1.18786488e-02 -1.67527199e-02  5.92461880e-03\n",
      "  1.72795891e-03 -9.03728139e-03 -1.77025760e-03  8.13582540e-03\n",
      "  8.89955275e-03 -1.76789593e-02  3.78633471e-04  1.27925584e-02\n",
      "  1.13191456e-02  5.72743686e-03 -7.13174371e-03  6.35361951e-03\n",
      "  2.12613530e-02 -2.25908402e-02  1.29367409e-05 -7.58284843e-03\n",
      "  5.32836234e-03 -1.49872871e-02  3.62101151e-03  2.57499097e-03\n",
      " -4.15318413e-03 -2.82202777e-03 -1.08713084e-05 -8.58881049e-06\n",
      " -2.17080087e-05 -7.15919351e-03  1.03551019e-02  6.22713473e-03\n",
      "  3.97742633e-03  8.15116055e-03  6.56501856e-03  6.05141930e-03\n",
      "  5.83709544e-03 -4.58540907e-03  7.44470395e-03 -4.68626246e-03]\n",
      "i [ 0.00992468  0.01038883 -0.0046013  -0.01090394  0.0020407   0.00421238\n",
      "  0.00850953 -0.00446648 -0.00557284 -0.00714291  0.00090039 -0.01832236\n",
      "  0.00491345 -0.02236168  0.00709031  0.00151081 -0.00651769  0.01913003\n",
      "  0.01029091 -0.01072484 -0.00363836 -0.00321025  0.00332859  0.01465668\n",
      " -0.00610558  0.00348401  0.0252513   0.01266808 -0.00508209 -0.00377956\n",
      "  0.00717329  0.01010406  0.00852275 -0.00431956  0.00636137 -0.01320202\n",
      " -0.00679527 -0.00697811 -0.00282879 -0.00252004 -0.00089935  0.00940582\n",
      " -0.00351777 -0.01322547  0.00520973 -0.01315601 -0.0042456  -0.00011819\n",
      "  0.00345165 -0.00899437  0.00468641 -0.00971292 -0.00742903  0.00143326\n",
      " -0.00358331  0.00740534  0.00380266  0.0087562   0.01030472 -0.00743579\n",
      " -0.01179063  0.01666761 -0.01931932  0.0098389   0.00869945 -0.00407728\n",
      " -0.01196185 -0.01997795  0.00891798 -0.00482179  0.00504655 -0.01353622\n",
      " -0.02052947 -0.00837001  0.01923338  0.0008387  -0.00100698  0.00479761\n",
      " -0.0154896  -0.00202672 -0.00489525  0.00486945  0.00838225  0.00662032\n",
      " -0.01165638 -0.01169508 -0.00145653 -0.00592093  0.00981951 -0.0154503\n",
      "  0.00988743  0.0053048   0.01248252 -0.01969691 -0.01535216 -0.01606605\n",
      "  0.01870719  0.01874676 -0.02717623 -0.00360181]\n",
      "hello [-0.00238526 -0.00480731 -0.00117159  0.00465766  0.01111997  0.00084898\n",
      "  0.01475856  0.0073023  -0.02379083  0.0133271   0.00254048 -0.00994809\n",
      " -0.00446089 -0.00356918 -0.01105551  0.00794924  0.007319   -0.00133034\n",
      "  0.01221605 -0.00280191  0.00233862 -0.00294285 -0.00385262  0.00268116\n",
      " -0.00387773 -0.00027803 -0.00132282  0.00766419  0.02309971  0.00596112\n",
      "  0.00564675 -0.00602508 -0.0041451  -0.0045264  -0.00238044  0.00574555\n",
      "  0.02132646 -0.01270258  0.01108805  0.00600279  0.00619428  0.00848046\n",
      " -0.02413302 -0.00871055 -0.01577267 -0.00858201  0.0042486  -0.01337107\n",
      "  0.0133332  -0.01689725 -0.02978069  0.01021155  0.00935663 -0.00103717\n",
      " -0.00274936  0.00307737 -0.00067001  0.00652652 -0.00932442 -0.00341119\n",
      "  0.01246518  0.00384429  0.01404517 -0.01446456  0.00401259  0.00711514\n",
      "  0.00783171 -0.000191   -0.00548891 -0.00111257 -0.00210725 -0.01035685\n",
      " -0.00447151  0.00382937 -0.01724085  0.00660805  0.0184117   0.00117179\n",
      " -0.01912948 -0.00766175 -0.00436529  0.01103967  0.00353975 -0.00924177\n",
      " -0.0014657  -0.00112521  0.0015164   0.01352061  0.01070604 -0.01047285\n",
      "  0.00278968  0.00862378 -0.00768208  0.00350901  0.01379757 -0.003814\n",
      "  0.00291436 -0.01971151  0.00117251  0.00762643]\n",
      ". [-0.00785563  0.00196777 -0.01570475  0.00195014  0.01249923  0.00452019\n",
      " -0.00475135  0.00014614 -0.00104067  0.01282977  0.0023093   0.01531927\n",
      "  0.01405302  0.01283573  0.00725814  0.00355883 -0.00620799 -0.00567545\n",
      " -0.00555034 -0.00169481 -0.00706834  0.00405335  0.00609149  0.00236728\n",
      "  0.01256043 -0.00496779 -0.01028715  0.01176458 -0.01213031  0.00750703\n",
      "  0.01391304 -0.01680288 -0.01719787 -0.00388828  0.00222585  0.0103861\n",
      "  0.00125142  0.01097797  0.00197069  0.02928133  0.01471279  0.01815901\n",
      "  0.01539006  0.00323691 -0.00415397 -0.0048197   0.01611887 -0.00299983\n",
      "  0.00031204  0.00439491 -0.00862629  0.00012365 -0.00386152 -0.01242277\n",
      " -0.00204585 -0.00301312  0.00056359 -0.00033494  0.01012796 -0.0024193\n",
      " -0.01095289 -0.00096346  0.01031899  0.00544913 -0.00598466  0.0210195\n",
      "  0.00202571  0.00374591  0.00315789  0.00526212 -0.00946604 -0.00311779\n",
      " -0.01147873  0.00856419 -0.01051588 -0.0072861   0.00317443  0.00941864\n",
      " -0.00782828 -0.00173    -0.00265341 -0.00703742 -0.00731677  0.01726\n",
      " -0.02723668  0.00201773 -0.00440452 -0.01833907 -0.01573879  0.00641148\n",
      "  0.0011173   0.01369338 -0.00734229  0.00750018  0.0033618  -0.00152419\n",
      " -0.00588371 -0.01921451  0.0041727  -0.00608248]\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "# if config.GPU:\n",
    "#     contexts, target = to_gpu(contexts), to_gpu(target)\n",
    "\n",
    "# モデルなどの生成\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "# 学習開始\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "# 後ほど利用できるように、必要なデータを保存\n",
    "word_vecs = model.word_vecs\n",
    "# if config.GPU:\n",
    "#     word_vecs = to_cpu(word_vecs)\n",
    "params = {}\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "# pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\n",
    "# with open(pkl_file, 'wb') as f:\n",
    "#     pickle.dump(params, f, -1)\n",
    "\n",
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 10\n",
    "h_prev = np.zeros([hidden_size])\n",
    "W_x = 0.01 * np.random.randn(word_vecs.shape[1], hidden_size)\n",
    "W_h = 0.01 * np.random.randn(hidden_size, hidden_size)\n",
    "b = 0.01 * np.random.randn(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10,), (100, 10), (10, 10), (10,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prev.shape, W_x.shape, W_h.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, h_prev):\n",
    "    h_out = []\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        h_next = np.tanh(np.dot(h_prev, W_h) + np.dot(x[i], W_x) + b)\n",
    "        h_out.append(h_next)\n",
    "        h_prev = h_next\n",
    "        \n",
    "    return np.array(h_out), h_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_next, h_prev = forward(word_vecs, h_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7, 10), (10,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_next.shape, h_prev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.95664274e-02, -2.09324045e-04, -5.96924093e-03,\n",
       "        -2.27484005e-03, -3.32324742e-03, -7.33908172e-03,\n",
       "         3.81491370e-03,  3.93533188e-03,  7.71466795e-04,\n",
       "        -1.48758733e-02],\n",
       "       [-1.94179491e-02, -4.71393344e-04, -4.01541350e-03,\n",
       "        -5.58006722e-03,  3.21274629e-04, -9.27452144e-03,\n",
       "         1.46344577e-03,  5.58347907e-03, -1.29208151e-03,\n",
       "        -1.30497539e-02],\n",
       "       [-1.88950954e-02, -5.80910445e-04, -5.32456909e-03,\n",
       "        -3.56671369e-03, -9.23975628e-04, -9.14573022e-03,\n",
       "         3.60613857e-03,  7.77298787e-03,  2.11530808e-03,\n",
       "        -1.55011711e-02],\n",
       "       [-1.70513064e-02, -7.71840524e-04, -5.48505027e-03,\n",
       "        -3.01534036e-03,  2.71612227e-05, -7.72786167e-03,\n",
       "         3.47155820e-03,  6.15896316e-03,  1.75862688e-03,\n",
       "        -1.31267233e-02],\n",
       "       [-1.86143812e-02,  1.51317911e-03, -7.00466982e-03,\n",
       "        -3.10101263e-03,  5.15950097e-04, -1.06179756e-02,\n",
       "         9.40335493e-04,  5.86725232e-03,  1.81596161e-03,\n",
       "        -1.40024644e-02],\n",
       "       [-1.84673874e-02, -3.13954177e-04, -3.58524079e-03,\n",
       "        -3.27341292e-03, -1.32243791e-03, -6.50218558e-03,\n",
       "         1.59017999e-03,  5.60103516e-03,  1.33241855e-03,\n",
       "        -1.35276315e-02],\n",
       "       [-1.80025783e-02, -1.81690913e-03, -3.46044769e-03,\n",
       "        -3.52527121e-03, -4.99523994e-04, -7.45565292e-03,\n",
       "         1.70410348e-03,  4.75064169e-03, -2.96158266e-04,\n",
       "        -1.21154294e-02]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW_h = 0.01 * np.random.randn(hidden_size)\n",
    "dh_next = np.zeros_like(h_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(dh_next, word_vecs):\n",
    "    for i in range(dh_next.shape[0]):\n",
    "        dt = dh_next[i] * (1 - h_next[i] ** 2)\n",
    "        db = np.sum(dt, axis=0)\n",
    "        dW_h = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, W_h.T)\n",
    "        dW_x = np.dot(word_vecs[i].T, dt)\n",
    "        dx = np.dot(dt, W_x.T)\n",
    "        \n",
    "    return dx, dh_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (100,) and (10,) not aligned: 100 (dim 0) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-070a13844f12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdh_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdh_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-31eafb856d31>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(dh_next, word_vecs)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdW_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdh_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdW_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (100,) and (10,) not aligned: 100 (dim 0) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "dx, dh_prev = backward(dh_next, word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここからはテキストを参考にクラス化を進めていきたいと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        h_next = np.tanh(np.dot(h_prev, Wh) + np.dot(x, Wx) + b)\n",
    "        self.cahe = (x, h_prev, h_next)\n",
    "        \n",
    "        return h_next\n",
    "    \n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "        \n",
    "        dt = dh_next - (1 - h_next**2)\n",
    "        db = np.sum(dt, axis=0)\n",
    "        dWh = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "        \n",
    "        self.grad[0][...] = dWx\n",
    "        self.grad[1][...] = dWh\n",
    "        self.grad[2][...] = db\n",
    "        \n",
    "        return dx, dh_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "        \n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "        \n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, D), dtype='f')\n",
    "        \n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "            \n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "            dxs[:, t, :] += dx\n",
    "            \n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "                \n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] += grad\n",
    "        self.dh = dh\n",
    "        \n",
    "        return dxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = None\n",
    "        self.W = W\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        N, T = xs.shape\n",
    "        V, D = self.W.shape\n",
    "        \n",
    "        out = np.empty((N, T, D), dtype='f')\n",
    "        self.layers = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = Embedding(self.W)\n",
    "            out[:, t, :] = layer.forward(xs[:, t])\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "        \n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "            \n",
    "        self.grads[0][...] = grad\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeAffine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        self.grads[0][...] += dW\n",
    "        self.grads[1][...] += db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "        \n",
    "        rx = x.reshape(N*T, -1)\n",
    "        out = np.dot(rx, W) + b\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "        \n",
    "        dout = dout.reshape(N*T, -1)\n",
    "        rx = x.reshape(N*T, -1)\n",
    "        \n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(rx.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dx = dx.reshape(*x.shape)\n",
    "        \n",
    "        self.grads[0][...] += dW\n",
    "        self.grads[1][...] += db\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeSoftmaxWithLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "        \n",
    "    return x\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "        \n",
    "    bacth_size = y.shape[0]\n",
    "    \n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-07)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.out = softmax(x)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = self.out * dout\n",
    "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
    "        dx -= self.out * sumdx\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        \n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "        \n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        \n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        self.ignore_label = -1\n",
    "        \n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "        \n",
    "        if ts.ndim == 3:\n",
    "            ts = ts.argmax(axis=2)\n",
    "            \n",
    "        mask = (ts != self.ignore_label)\n",
    "        \n",
    "        xs = xs.reshape(N*T, V)\n",
    "        ts = ts.reshape(N*T)\n",
    "        mask = mask.reshape(N*T)\n",
    "        \n",
    "        ys = softmax(xs)\n",
    "        ls = np.log(ys[np.arange(N*T), ts])\n",
    "        ls *= mask\n",
    "        loss = -np.sum(ls)\n",
    "        loss /= mask.sum()\n",
    "        \n",
    "        self.cache = (ts, ys, mask, (N, T, V))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        ts, ys, mask, (N, T, V) = self.cache\n",
    "        \n",
    "        dx = ys\n",
    "        dx[np.arange(N*T), ts] -= 1\n",
    "        dx *= dout\n",
    "        dx /= mask.sum()\n",
    "        dx *= mask[:, np.newaxis]\n",
    "        \n",
    "        dx = dx.reshape((N, T, V))\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRnnlm:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        rnn_b = np.zeros(H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W), \n",
    "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True), \n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.rnn_layer = self.layers[1]\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    def forward(self, xs, ts):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in self.layers:\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('./deep-learning-from-scratch-2')\n",
    "from common.optimizer import SGD\n",
    "from dataset import ptb\n",
    "from common.layers import Affine, SoftmaxWithLoss, Embedding\n",
    "from common.time_layers import RNN, TimeRNN, TimeEmbedding, TimeAffine, TimeSoftmaxWithLoss\n",
    "from ch05.simple_rnnlm import SimpleRnnlm\n",
    "from common.functions import softmax, cross_entropy_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100\n",
    "time_size = 5\n",
    "lr = 0.1\n",
    "max_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_size = 1000\n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 1000, vovabulary size: 418\n"
     ]
    }
   ],
   "source": [
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "data_size = len(xs)\n",
    "print('corpus size: %d, vovabulary size: %d' % (corpus_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = data_size // (batch_size * time_size)\n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "ppl_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "｜epoch 1 ｜perplexity 371.23\n",
      "｜epoch 2 ｜perplexity 249.48\n",
      "｜epoch 3 ｜perplexity 222.42\n",
      "｜epoch 4 ｜perplexity 214.06\n",
      "｜epoch 5 ｜perplexity 204.84\n",
      "｜epoch 6 ｜perplexity 202.48\n",
      "｜epoch 7 ｜perplexity 197.60\n",
      "｜epoch 8 ｜perplexity 196.25\n",
      "｜epoch 9 ｜perplexity 191.19\n",
      "｜epoch 10 ｜perplexity 192.14\n",
      "｜epoch 11 ｜perplexity 188.65\n",
      "｜epoch 12 ｜perplexity 191.73\n",
      "｜epoch 13 ｜perplexity 189.42\n",
      "｜epoch 14 ｜perplexity 189.89\n",
      "｜epoch 15 ｜perplexity 188.89\n",
      "｜epoch 16 ｜perplexity 185.25\n",
      "｜epoch 17 ｜perplexity 183.37\n",
      "｜epoch 18 ｜perplexity 179.47\n",
      "｜epoch 19 ｜perplexity 180.41\n",
      "｜epoch 20 ｜perplexity 181.61\n",
      "｜epoch 21 ｜perplexity 179.35\n",
      "｜epoch 22 ｜perplexity 174.48\n",
      "｜epoch 23 ｜perplexity 170.44\n",
      "｜epoch 24 ｜perplexity 171.17\n",
      "｜epoch 25 ｜perplexity 168.80\n",
      "｜epoch 26 ｜perplexity 167.14\n",
      "｜epoch 27 ｜perplexity 160.51\n",
      "｜epoch 28 ｜perplexity 158.33\n",
      "｜epoch 29 ｜perplexity 155.56\n",
      "｜epoch 30 ｜perplexity 149.01\n",
      "｜epoch 31 ｜perplexity 148.22\n",
      "｜epoch 32 ｜perplexity 144.11\n",
      "｜epoch 33 ｜perplexity 140.81\n",
      "｜epoch 34 ｜perplexity 135.77\n",
      "｜epoch 35 ｜perplexity 134.83\n",
      "｜epoch 36 ｜perplexity 127.33\n",
      "｜epoch 37 ｜perplexity 120.95\n",
      "｜epoch 38 ｜perplexity 117.76\n",
      "｜epoch 39 ｜perplexity 113.76\n",
      "｜epoch 40 ｜perplexity 109.90\n",
      "｜epoch 41 ｜perplexity 107.29\n",
      "｜epoch 42 ｜perplexity 101.17\n",
      "｜epoch 43 ｜perplexity 94.71\n",
      "｜epoch 44 ｜perplexity 90.87\n",
      "｜epoch 45 ｜perplexity 88.61\n",
      "｜epoch 46 ｜perplexity 86.94\n",
      "｜epoch 47 ｜perplexity 79.82\n",
      "｜epoch 48 ｜perplexity 76.09\n",
      "｜epoch 49 ｜perplexity 73.57\n",
      "｜epoch 50 ｜perplexity 70.22\n",
      "｜epoch 51 ｜perplexity 65.55\n",
      "｜epoch 52 ｜perplexity 62.65\n",
      "｜epoch 53 ｜perplexity 59.77\n",
      "｜epoch 54 ｜perplexity 56.32\n",
      "｜epoch 55 ｜perplexity 53.93\n",
      "｜epoch 56 ｜perplexity 49.95\n",
      "｜epoch 57 ｜perplexity 46.99\n",
      "｜epoch 58 ｜perplexity 45.19\n",
      "｜epoch 59 ｜perplexity 41.96\n",
      "｜epoch 60 ｜perplexity 40.43\n",
      "｜epoch 61 ｜perplexity 38.16\n",
      "｜epoch 62 ｜perplexity 36.43\n",
      "｜epoch 63 ｜perplexity 33.59\n",
      "｜epoch 64 ｜perplexity 30.84\n",
      "｜epoch 65 ｜perplexity 30.13\n",
      "｜epoch 66 ｜perplexity 28.59\n",
      "｜epoch 67 ｜perplexity 27.92\n",
      "｜epoch 68 ｜perplexity 25.14\n",
      "｜epoch 69 ｜perplexity 24.54\n",
      "｜epoch 70 ｜perplexity 22.95\n",
      "｜epoch 71 ｜perplexity 21.24\n",
      "｜epoch 72 ｜perplexity 20.82\n",
      "｜epoch 73 ｜perplexity 18.67\n",
      "｜epoch 74 ｜perplexity 18.14\n",
      "｜epoch 75 ｜perplexity 17.10\n",
      "｜epoch 76 ｜perplexity 15.72\n",
      "｜epoch 77 ｜perplexity 15.23\n",
      "｜epoch 78 ｜perplexity 14.23\n",
      "｜epoch 79 ｜perplexity 13.46\n",
      "｜epoch 80 ｜perplexity 12.89\n",
      "｜epoch 81 ｜perplexity 12.50\n",
      "｜epoch 82 ｜perplexity 12.13\n",
      "｜epoch 83 ｜perplexity 10.85\n",
      "｜epoch 84 ｜perplexity 10.61\n",
      "｜epoch 85 ｜perplexity 10.06\n",
      "｜epoch 86 ｜perplexity 9.55\n",
      "｜epoch 87 ｜perplexity 9.11\n",
      "｜epoch 88 ｜perplexity 8.98\n",
      "｜epoch 89 ｜perplexity 8.41\n",
      "｜epoch 90 ｜perplexity 7.77\n",
      "｜epoch 91 ｜perplexity 7.37\n",
      "｜epoch 92 ｜perplexity 6.99\n",
      "｜epoch 93 ｜perplexity 6.74\n",
      "｜epoch 94 ｜perplexity 6.36\n",
      "｜epoch 95 ｜perplexity 6.25\n",
      "｜epoch 96 ｜perplexity 6.00\n",
      "｜epoch 97 ｜perplexity 5.87\n",
      "｜epoch 98 ｜perplexity 5.69\n",
      "｜epoch 99 ｜perplexity 5.50\n",
      "｜epoch 100 ｜perplexity 5.26\n"
     ]
    }
   ],
   "source": [
    "jump = (corpus_size - 1) // batch_size\n",
    "offsets = [i * jump for i in range(batch_size)]\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for iter in range(max_iters):\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "            time_idx += 1\n",
    "            \n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "        \n",
    "    ppl = np.exp(total_loss / loss_count)\n",
    "    print('｜epoch %d ｜perplexity %.2f' % (epoch+1, ppl))\n",
    "    ppl_list.append(float(ppl))\n",
    "    total_loss, loss_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# グラフの描画\n",
    "x = np.arange(len(ppl_list))\n",
    "plt.plot(x, ppl_list, label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('perplexity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "209px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
