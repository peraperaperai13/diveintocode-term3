{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attentionの仕組み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seqの問題点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2se2の問題点はどんな長さの入力文でも固定長のベクトルにしてしまうことです。  \n",
    "これでは必要な情報がうまく伝達できないのです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoderの改良"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そのため、Encoderの出力は入力される文章の長さに応じて、その長さを変えるべきです。  \n",
    "つまり、可変長のベクトルにするために、LSTMレイヤの隠れ状態のベクトルを全て利用するのです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoderの改良"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoderの出力を全て使う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先ほど、Encoderの改良の部分でLSTMレイヤの隠れ状態のベクトル（hsと呼びます）を全て使うと言いましたが、  \n",
    "そのベクトルをDecoderで使用していきます。  \n",
    "今まではhsの最後の行のみをDecoderに渡していましたが、これを全て渡していくことが1つめの改良です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "print(ar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "t = hs * ar\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "c = np.sum(t, axis=0)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n"
     ]
    }
   ],
   "source": [
    "t = hs * ar\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "c = np.sum(t, axis=1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ar = a.reshape(N, T, 1)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "        \n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでhsを全て利用する準備が整いました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各単語の重要度を表す重み\"a\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EncoderのLSTMレイヤの出力全てをhsと言っていました。  \n",
    "DecoderのLSTMレイヤの1つの出力をhとすると、hsの中からhに似ているベクトルを探すということをします。  \n",
    "それにはhsとhで内積をとります。これで各単語の重要度を表す重みを得ることができるのです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./deep-learning-from-scratch-2')\n",
    "from common.layers import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n"
     ]
    }
   ],
   "source": [
    "t = hs * ar\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "s = np.sum(t, axis=2)\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        N , T, H = hs.shape\n",
    "        \n",
    "        hr = h.reshape(N, 1, H)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "        \n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, 1, H).repeat(N, T, H)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "        \n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで各単語の重要度の重みを取ってくる準備ができました。最後に1つめの改良と今回の改良を組み合わせます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attentionレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この組み合わせがまさにAttentionの核心です。それではAttentionレイヤを実装します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.foward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "    \n",
    "    def backward(self, da):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでAttentionレイヤが実装できました。次はTimeAttentionレイヤを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.parmas, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weight = None\n",
    "        \n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:, t, :] = dh\n",
    "            \n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでTimeAttentionを実装することができました。次はAttentionを使って学習を行なっていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AttentionEncoderの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.time_layers import *\n",
    "from ch07.seq2seq import Encoder, Seq2seq\n",
    "from ch08.attention_layer import TimeAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AttentionDecoderの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2 * H, V) / np.sqrt(2 * H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "        \n",
    "        dc, ddec_hs0 = dout[:, :, :H], dout[:, :, H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "        \n",
    "        return denc_hs\n",
    "    \n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "            \n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "            \n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "        \n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AttentionSeq2seqの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ch07.seq2seq import Encoder, Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from ch08.attention_seq2seq import AttentionSeq2seq\n",
    "from ch07.seq2seq import Seq2seq\n",
    "from ch07.peeky_seq2seq import PeekySeq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 4.08\n",
      "| epoch 1 |  iter 21 / 351 | time 7[s] | loss 3.09\n",
      "| epoch 1 |  iter 41 / 351 | time 14[s] | loss 1.90\n",
      "| epoch 1 |  iter 61 / 351 | time 21[s] | loss 1.72\n",
      "| epoch 1 |  iter 81 / 351 | time 28[s] | loss 1.46\n",
      "| epoch 1 |  iter 101 / 351 | time 35[s] | loss 1.19\n",
      "| epoch 1 |  iter 121 / 351 | time 42[s] | loss 1.14\n",
      "| epoch 1 |  iter 141 / 351 | time 49[s] | loss 1.09\n",
      "| epoch 1 |  iter 161 / 351 | time 56[s] | loss 1.06\n",
      "| epoch 1 |  iter 181 / 351 | time 63[s] | loss 1.04\n",
      "| epoch 1 |  iter 201 / 351 | time 70[s] | loss 1.03\n",
      "| epoch 1 |  iter 221 / 351 | time 77[s] | loss 1.02\n",
      "| epoch 1 |  iter 241 / 351 | time 84[s] | loss 1.02\n",
      "| epoch 1 |  iter 261 / 351 | time 92[s] | loss 1.01\n",
      "| epoch 1 |  iter 281 / 351 | time 99[s] | loss 1.00\n",
      "| epoch 1 |  iter 301 / 351 | time 106[s] | loss 1.00\n",
      "| epoch 1 |  iter 321 / 351 | time 113[s] | loss 1.00\n",
      "| epoch 1 |  iter 341 / 351 | time 120[s] | loss 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "val acc 0.000%\n",
      "| epoch 2 |  iter 1 / 351 | time 0[s] | loss 1.00\n",
      "| epoch 2 |  iter 21 / 351 | time 7[s] | loss 1.00\n",
      "| epoch 2 |  iter 41 / 351 | time 14[s] | loss 0.99\n",
      "| epoch 2 |  iter 61 / 351 | time 21[s] | loss 0.99\n",
      "| epoch 2 |  iter 81 / 351 | time 28[s] | loss 0.99\n",
      "| epoch 2 |  iter 101 / 351 | time 35[s] | loss 0.99\n",
      "| epoch 2 |  iter 121 / 351 | time 42[s] | loss 0.99\n",
      "| epoch 2 |  iter 141 / 351 | time 49[s] | loss 0.98\n",
      "| epoch 2 |  iter 161 / 351 | time 56[s] | loss 0.98\n",
      "| epoch 2 |  iter 181 / 351 | time 63[s] | loss 0.97\n",
      "| epoch 2 |  iter 201 / 351 | time 70[s] | loss 0.95\n",
      "| epoch 2 |  iter 221 / 351 | time 78[s] | loss 0.94\n",
      "| epoch 2 |  iter 241 / 351 | time 85[s] | loss 0.90\n",
      "| epoch 2 |  iter 261 / 351 | time 92[s] | loss 0.83\n",
      "| epoch 2 |  iter 281 / 351 | time 99[s] | loss 0.74\n",
      "| epoch 2 |  iter 301 / 351 | time 106[s] | loss 0.66\n",
      "| epoch 2 |  iter 321 / 351 | time 112[s] | loss 0.58\n",
      "| epoch 2 |  iter 341 / 351 | time 119[s] | loss 0.47\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 2016-11-08\n",
      "---\n",
      "val acc 51.460%\n",
      "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 0.35\n",
      "| epoch 3 |  iter 21 / 351 | time 7[s] | loss 0.30\n",
      "| epoch 3 |  iter 41 / 351 | time 14[s] | loss 0.21\n",
      "| epoch 3 |  iter 61 / 351 | time 21[s] | loss 0.14\n",
      "| epoch 3 |  iter 81 / 351 | time 28[s] | loss 0.09\n",
      "| epoch 3 |  iter 101 / 351 | time 35[s] | loss 0.07\n",
      "| epoch 3 |  iter 121 / 351 | time 42[s] | loss 0.05\n",
      "| epoch 3 |  iter 141 / 351 | time 49[s] | loss 0.04\n",
      "| epoch 3 |  iter 161 / 351 | time 56[s] | loss 0.03\n",
      "| epoch 3 |  iter 181 / 351 | time 63[s] | loss 0.03\n",
      "| epoch 3 |  iter 201 / 351 | time 70[s] | loss 0.02\n",
      "| epoch 3 |  iter 221 / 351 | time 77[s] | loss 0.02\n",
      "| epoch 3 |  iter 241 / 351 | time 84[s] | loss 0.02\n",
      "| epoch 3 |  iter 261 / 351 | time 91[s] | loss 0.01\n",
      "| epoch 3 |  iter 281 / 351 | time 97[s] | loss 0.01\n",
      "| epoch 3 |  iter 301 / 351 | time 104[s] | loss 0.01\n",
      "| epoch 3 |  iter 321 / 351 | time 111[s] | loss 0.01\n",
      "| epoch 3 |  iter 341 / 351 | time 118[s] | loss 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 99.900%\n",
      "| epoch 4 |  iter 1 / 351 | time 0[s] | loss 0.01\n",
      "| epoch 4 |  iter 21 / 351 | time 7[s] | loss 0.01\n",
      "| epoch 4 |  iter 41 / 351 | time 14[s] | loss 0.01\n",
      "| epoch 4 |  iter 61 / 351 | time 21[s] | loss 0.01\n",
      "| epoch 4 |  iter 81 / 351 | time 27[s] | loss 0.01\n",
      "| epoch 4 |  iter 101 / 351 | time 34[s] | loss 0.01\n",
      "| epoch 4 |  iter 121 / 351 | time 41[s] | loss 0.00\n",
      "| epoch 4 |  iter 141 / 351 | time 48[s] | loss 0.01\n",
      "| epoch 4 |  iter 161 / 351 | time 55[s] | loss 0.00\n",
      "| epoch 4 |  iter 181 / 351 | time 62[s] | loss 0.00\n",
      "| epoch 4 |  iter 201 / 351 | time 69[s] | loss 0.00\n",
      "| epoch 4 |  iter 221 / 351 | time 76[s] | loss 0.00\n",
      "| epoch 4 |  iter 241 / 351 | time 82[s] | loss 0.00\n",
      "| epoch 4 |  iter 261 / 351 | time 89[s] | loss 0.00\n",
      "| epoch 4 |  iter 281 / 351 | time 96[s] | loss 0.01\n",
      "| epoch 4 |  iter 301 / 351 | time 103[s] | loss 0.00\n",
      "| epoch 4 |  iter 321 / 351 | time 110[s] | loss 0.00\n",
      "| epoch 4 |  iter 341 / 351 | time 117[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 99.900%\n",
      "| epoch 5 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 5 |  iter 21 / 351 | time 7[s] | loss 0.00\n",
      "| epoch 5 |  iter 41 / 351 | time 14[s] | loss 0.00\n",
      "| epoch 5 |  iter 61 / 351 | time 21[s] | loss 0.00\n",
      "| epoch 5 |  iter 81 / 351 | time 27[s] | loss 0.00\n",
      "| epoch 5 |  iter 101 / 351 | time 34[s] | loss 0.00\n",
      "| epoch 5 |  iter 121 / 351 | time 41[s] | loss 0.00\n",
      "| epoch 5 |  iter 141 / 351 | time 48[s] | loss 0.00\n",
      "| epoch 5 |  iter 161 / 351 | time 55[s] | loss 0.00\n",
      "| epoch 5 |  iter 181 / 351 | time 62[s] | loss 0.00\n",
      "| epoch 5 |  iter 201 / 351 | time 68[s] | loss 0.00\n",
      "| epoch 5 |  iter 221 / 351 | time 75[s] | loss 0.00\n",
      "| epoch 5 |  iter 241 / 351 | time 82[s] | loss 0.00\n",
      "| epoch 5 |  iter 261 / 351 | time 89[s] | loss 0.00\n",
      "| epoch 5 |  iter 281 / 351 | time 96[s] | loss 0.00\n",
      "| epoch 5 |  iter 301 / 351 | time 103[s] | loss 0.00\n",
      "| epoch 5 |  iter 321 / 351 | time 110[s] | loss 0.00\n",
      "| epoch 5 |  iter 341 / 351 | time 117[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1263a13a3ca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         correct_num += eval_seq2seq(model, question, correct,\n\u001b[0;32m---> 33\u001b[0;31m                                     id_to_char, verbose, is_reverse=True)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_num\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML/sprint23/deep-learning-from-scratch-2/common/util.py\u001b[0m in \u001b[0;36meval_seq2seq\u001b[0;34m(model, question, correct, id_to_char, verbos, is_reverse)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mstart_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0mguess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;31m# 文字列へ変換\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML/sprint23/deep-learning-from-scratch-2/ch07/seq2seq.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, xs, start_id, sample_size)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0msampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msampled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML/sprint23/deep-learning-from-scratch-2/ch08/attention_seq2seq.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML/sprint23/deep-learning-from-scratch-2/common/time_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML/sprint23/deep-learning-from-scratch-2/common/time_layers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, Wx, Wh, b)\u001b[0m\n\u001b[1;32m    105\u001b[0m         '''\n\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mWx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mzeros_like\u001b[0;34m(a, dtype, order, subok)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;31m# needed instead of a 0 to get same result as zeros for for string dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unsafe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 入力文を反転\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# グラフの描画\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 4.08\n",
      "| epoch 1 |  iter 21 / 351 | time 4[s] | loss 3.34\n",
      "| epoch 1 |  iter 41 / 351 | time 8[s] | loss 1.95\n",
      "| epoch 1 |  iter 61 / 351 | time 12[s] | loss 1.78\n",
      "| epoch 1 |  iter 81 / 351 | time 16[s] | loss 1.70\n",
      "| epoch 1 |  iter 101 / 351 | time 20[s] | loss 1.55\n",
      "| epoch 1 |  iter 121 / 351 | time 24[s] | loss 1.27\n",
      "| epoch 1 |  iter 141 / 351 | time 28[s] | loss 1.16\n",
      "| epoch 1 |  iter 161 / 351 | time 32[s] | loss 1.12\n",
      "| epoch 1 |  iter 181 / 351 | time 36[s] | loss 1.08\n",
      "| epoch 1 |  iter 201 / 351 | time 40[s] | loss 1.06\n",
      "| epoch 1 |  iter 221 / 351 | time 44[s] | loss 1.05\n",
      "| epoch 1 |  iter 241 / 351 | time 48[s] | loss 1.04\n",
      "| epoch 1 |  iter 261 / 351 | time 52[s] | loss 1.04\n",
      "| epoch 1 |  iter 281 / 351 | time 55[s] | loss 1.03\n",
      "| epoch 1 |  iter 301 / 351 | time 59[s] | loss 1.03\n",
      "| epoch 1 |  iter 321 / 351 | time 63[s] | loss 1.03\n",
      "| epoch 1 |  iter 341 / 351 | time 67[s] | loss 1.02\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1999-01-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 1999-01-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 1999-01-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 1999-01-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1999-01-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1999-01-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1999-01-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1999-01-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1999-01-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 1999-01-11\n",
      "---\n",
      "val acc 0.000%\n",
      "| epoch 2 |  iter 1 / 351 | time 0[s] | loss 1.03\n",
      "| epoch 2 |  iter 21 / 351 | time 4[s] | loss 1.02\n",
      "| epoch 2 |  iter 41 / 351 | time 8[s] | loss 1.01\n",
      "| epoch 2 |  iter 61 / 351 | time 12[s] | loss 1.01\n",
      "| epoch 2 |  iter 81 / 351 | time 16[s] | loss 1.00\n",
      "| epoch 2 |  iter 101 / 351 | time 19[s] | loss 1.00\n",
      "| epoch 2 |  iter 121 / 351 | time 23[s] | loss 1.00\n",
      "| epoch 2 |  iter 141 / 351 | time 27[s] | loss 1.00\n",
      "| epoch 2 |  iter 161 / 351 | time 31[s] | loss 1.00\n",
      "| epoch 2 |  iter 181 / 351 | time 35[s] | loss 1.00\n",
      "| epoch 2 |  iter 201 / 351 | time 39[s] | loss 0.99\n",
      "| epoch 2 |  iter 221 / 351 | time 43[s] | loss 0.99\n",
      "| epoch 2 |  iter 241 / 351 | time 47[s] | loss 0.99\n",
      "| epoch 2 |  iter 261 / 351 | time 51[s] | loss 0.99\n",
      "| epoch 2 |  iter 281 / 351 | time 54[s] | loss 0.99\n",
      "| epoch 2 |  iter 301 / 351 | time 58[s] | loss 0.99\n",
      "| epoch 2 |  iter 321 / 351 | time 62[s] | loss 0.99\n",
      "| epoch 2 |  iter 341 / 351 | time 66[s] | loss 0.99\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1982-04-21\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 1982-04-21\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 1982-04-21\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 1982-04-21\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1982-04-21\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1982-04-21\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1982-04-21\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1982-04-21\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1982-04-21\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 1982-04-21\n",
      "---\n",
      "val acc 0.020%\n",
      "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 0.99\n",
      "| epoch 3 |  iter 21 / 351 | time 4[s] | loss 0.99\n",
      "| epoch 3 |  iter 41 / 351 | time 8[s] | loss 0.99\n",
      "| epoch 3 |  iter 61 / 351 | time 12[s] | loss 0.99\n",
      "| epoch 3 |  iter 81 / 351 | time 16[s] | loss 0.98\n",
      "| epoch 3 |  iter 101 / 351 | time 20[s] | loss 0.99\n",
      "| epoch 3 |  iter 121 / 351 | time 24[s] | loss 0.99\n",
      "| epoch 3 |  iter 141 / 351 | time 28[s] | loss 0.99\n",
      "| epoch 3 |  iter 161 / 351 | time 33[s] | loss 0.99\n",
      "| epoch 3 |  iter 181 / 351 | time 37[s] | loss 0.99\n",
      "| epoch 3 |  iter 201 / 351 | time 41[s] | loss 0.99\n",
      "| epoch 3 |  iter 221 / 351 | time 45[s] | loss 0.99\n",
      "| epoch 3 |  iter 241 / 351 | time 49[s] | loss 0.99\n",
      "| epoch 3 |  iter 261 / 351 | time 53[s] | loss 0.99\n",
      "| epoch 3 |  iter 281 / 351 | time 57[s] | loss 0.99\n",
      "| epoch 3 |  iter 301 / 351 | time 61[s] | loss 0.98\n",
      "| epoch 3 |  iter 321 / 351 | time 65[s] | loss 0.98\n",
      "| epoch 3 |  iter 341 / 351 | time 69[s] | loss 0.98\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1999-04-14\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 1999-04-14\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 1999-04-14\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 1999-04-14\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1999-04-14\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1999-04-14\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1999-04-14\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1999-04-14\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1999-04-14\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 1999-04-14\n",
      "---\n",
      "val acc 0.000%\n",
      "| epoch 4 |  iter 1 / 351 | time 0[s] | loss 0.98\n",
      "| epoch 4 |  iter 21 / 351 | time 4[s] | loss 0.99\n",
      "| epoch 4 |  iter 41 / 351 | time 7[s] | loss 0.98\n",
      "| epoch 4 |  iter 61 / 351 | time 11[s] | loss 0.98\n",
      "| epoch 4 |  iter 81 / 351 | time 15[s] | loss 0.98\n",
      "| epoch 4 |  iter 101 / 351 | time 19[s] | loss 0.98\n",
      "| epoch 4 |  iter 121 / 351 | time 23[s] | loss 0.98\n",
      "| epoch 4 |  iter 141 / 351 | time 27[s] | loss 0.98\n",
      "| epoch 4 |  iter 161 / 351 | time 30[s] | loss 0.98\n",
      "| epoch 4 |  iter 181 / 351 | time 34[s] | loss 0.98\n",
      "| epoch 4 |  iter 201 / 351 | time 38[s] | loss 0.98\n",
      "| epoch 4 |  iter 221 / 351 | time 42[s] | loss 0.98\n",
      "| epoch 4 |  iter 241 / 351 | time 46[s] | loss 0.98\n",
      "| epoch 4 |  iter 261 / 351 | time 50[s] | loss 0.98\n",
      "| epoch 4 |  iter 281 / 351 | time 53[s] | loss 0.98\n",
      "| epoch 4 |  iter 301 / 351 | time 57[s] | loss 0.98\n",
      "| epoch 4 |  iter 321 / 351 | time 61[s] | loss 0.98\n",
      "| epoch 4 |  iter 341 / 351 | time 65[s] | loss 0.98\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1976-01-17\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 1976-01-17\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 1976-01-17\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 1976-01-17\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1976-01-17\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1976-01-17\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1976-01-17\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1976-01-17\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1976-01-17\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 1976-01-17\n",
      "---\n",
      "val acc 0.020%\n",
      "| epoch 5 |  iter 1 / 351 | time 0[s] | loss 0.98\n",
      "| epoch 5 |  iter 21 / 351 | time 4[s] | loss 0.98\n",
      "| epoch 5 |  iter 41 / 351 | time 7[s] | loss 0.98\n",
      "| epoch 5 |  iter 61 / 351 | time 11[s] | loss 0.98\n",
      "| epoch 5 |  iter 81 / 351 | time 15[s] | loss 0.98\n",
      "| epoch 5 |  iter 101 / 351 | time 19[s] | loss 0.98\n",
      "| epoch 5 |  iter 121 / 351 | time 23[s] | loss 0.98\n",
      "| epoch 5 |  iter 141 / 351 | time 27[s] | loss 0.98\n",
      "| epoch 5 |  iter 161 / 351 | time 30[s] | loss 0.98\n",
      "| epoch 5 |  iter 181 / 351 | time 34[s] | loss 0.98\n",
      "| epoch 5 |  iter 201 / 351 | time 38[s] | loss 0.98\n",
      "| epoch 5 |  iter 221 / 351 | time 42[s] | loss 0.98\n",
      "| epoch 5 |  iter 241 / 351 | time 46[s] | loss 0.98\n",
      "| epoch 5 |  iter 261 / 351 | time 50[s] | loss 0.98\n",
      "| epoch 5 |  iter 281 / 351 | time 54[s] | loss 0.98\n",
      "| epoch 5 |  iter 301 / 351 | time 57[s] | loss 0.98\n",
      "| epoch 5 |  iter 321 / 351 | time 61[s] | loss 0.98\n",
      "| epoch 5 |  iter 341 / 351 | time 65[s] | loss 0.98\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1994-08-29\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 1994-08-29\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 1994-08-29\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 1994-08-29\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1994-08-29\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1994-08-29\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1994-08-29\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1994-08-29\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1994-08-29\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 1994-08-29\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 0.020%\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 入力文を反転\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 5\n",
    "max_grad = 5.0\n",
    "\n",
    "# model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "y_acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    y_acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# グラフの描画\n",
    "y = np.arange(len(y_acc_list))\n",
    "plt.plot(y, y_acc_list, marker='o')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peeky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 4.08\n",
      "| epoch 1 |  iter 21 / 351 | time 4[s] | loss 2.86\n",
      "| epoch 1 |  iter 41 / 351 | time 9[s] | loss 1.89\n",
      "| epoch 1 |  iter 61 / 351 | time 13[s] | loss 1.78\n",
      "| epoch 1 |  iter 81 / 351 | time 17[s] | loss 1.70\n",
      "| epoch 1 |  iter 101 / 351 | time 22[s] | loss 1.57\n",
      "| epoch 1 |  iter 121 / 351 | time 26[s] | loss 1.30\n",
      "| epoch 1 |  iter 141 / 351 | time 30[s] | loss 1.16\n",
      "| epoch 1 |  iter 161 / 351 | time 35[s] | loss 1.10\n",
      "| epoch 1 |  iter 181 / 351 | time 39[s] | loss 1.07\n",
      "| epoch 1 |  iter 201 / 351 | time 43[s] | loss 1.05\n",
      "| epoch 1 |  iter 221 / 351 | time 47[s] | loss 1.04\n",
      "| epoch 1 |  iter 241 / 351 | time 52[s] | loss 1.04\n",
      "| epoch 1 |  iter 261 / 351 | time 56[s] | loss 1.03\n",
      "| epoch 1 |  iter 281 / 351 | time 60[s] | loss 1.02\n",
      "| epoch 1 |  iter 301 / 351 | time 65[s] | loss 1.01\n",
      "| epoch 1 |  iter 321 / 351 | time 69[s] | loss 1.00\n",
      "| epoch 1 |  iter 341 / 351 | time 73[s] | loss 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1971-11-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 1973-01-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 1983-03-03\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 1973-01-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1973-04-09\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1971-11-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1983-04-09\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1983-04-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1971-11-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 1996-01-11\n",
      "---\n",
      "val acc 0.020%\n",
      "| epoch 2 |  iter 1 / 351 | time 0[s] | loss 0.98\n",
      "| epoch 2 |  iter 21 / 351 | time 4[s] | loss 0.98\n",
      "| epoch 2 |  iter 41 / 351 | time 8[s] | loss 0.97\n",
      "| epoch 2 |  iter 61 / 351 | time 13[s] | loss 0.96\n",
      "| epoch 2 |  iter 81 / 351 | time 17[s] | loss 0.95\n",
      "| epoch 2 |  iter 101 / 351 | time 21[s] | loss 0.93\n",
      "| epoch 2 |  iter 121 / 351 | time 26[s] | loss 0.92\n",
      "| epoch 2 |  iter 141 / 351 | time 30[s] | loss 0.91\n",
      "| epoch 2 |  iter 161 / 351 | time 34[s] | loss 0.90\n",
      "| epoch 2 |  iter 181 / 351 | time 38[s] | loss 0.89\n",
      "| epoch 2 |  iter 201 / 351 | time 43[s] | loss 0.88\n",
      "| epoch 2 |  iter 221 / 351 | time 47[s] | loss 0.87\n",
      "| epoch 2 |  iter 241 / 351 | time 51[s] | loss 0.88\n",
      "| epoch 2 |  iter 261 / 351 | time 56[s] | loss 0.84\n",
      "| epoch 2 |  iter 281 / 351 | time 60[s] | loss 0.81\n",
      "| epoch 2 |  iter 301 / 351 | time 64[s] | loss 0.80\n",
      "| epoch 2 |  iter 321 / 351 | time 69[s] | loss 0.77\n",
      "| epoch 2 |  iter 341 / 351 | time 73[s] | loss 0.75\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1984-11-04\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 2011-11-24\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 2005-03-23\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 2011-12-13\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1971-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 2004-10-21\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1992-04-29\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1990-08-04\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 2004-10-24\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 2011-10-14\n",
      "---\n",
      "val acc 0.560%\n",
      "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 0.71\n",
      "| epoch 3 |  iter 21 / 351 | time 4[s] | loss 0.72\n",
      "| epoch 3 |  iter 41 / 351 | time 8[s] | loss 0.69\n",
      "| epoch 3 |  iter 61 / 351 | time 13[s] | loss 0.66\n",
      "| epoch 3 |  iter 81 / 351 | time 17[s] | loss 0.63\n",
      "| epoch 3 |  iter 101 / 351 | time 21[s] | loss 0.60\n",
      "| epoch 3 |  iter 121 / 351 | time 26[s] | loss 0.56\n",
      "| epoch 3 |  iter 141 / 351 | time 30[s] | loss 0.56\n",
      "| epoch 3 |  iter 161 / 351 | time 34[s] | loss 0.53\n",
      "| epoch 3 |  iter 181 / 351 | time 39[s] | loss 0.50\n",
      "| epoch 3 |  iter 201 / 351 | time 43[s] | loss 0.48\n",
      "| epoch 3 |  iter 221 / 351 | time 47[s] | loss 0.45\n",
      "| epoch 3 |  iter 241 / 351 | time 52[s] | loss 0.42\n",
      "| epoch 3 |  iter 261 / 351 | time 56[s] | loss 0.39\n",
      "| epoch 3 |  iter 281 / 351 | time 60[s] | loss 0.35\n",
      "| epoch 3 |  iter 301 / 351 | time 65[s] | loss 0.33\n",
      "| epoch 3 |  iter 321 / 351 | time 69[s] | loss 0.30\n",
      "| epoch 3 |  iter 341 / 351 | time 73[s] | loss 0.28\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1985-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 2008-11-10\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 2016-12-12\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1970-04-17\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 2007-08-20\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 2013-10-05\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 35.900%\n",
      "| epoch 4 |  iter 1 / 351 | time 0[s] | loss 0.26\n",
      "| epoch 4 |  iter 21 / 351 | time 4[s] | loss 0.24\n",
      "| epoch 4 |  iter 41 / 351 | time 8[s] | loss 0.21\n",
      "| epoch 4 |  iter 61 / 351 | time 13[s] | loss 0.20\n",
      "| epoch 4 |  iter 81 / 351 | time 17[s] | loss 0.17\n",
      "| epoch 4 |  iter 101 / 351 | time 21[s] | loss 0.16\n",
      "| epoch 4 |  iter 121 / 351 | time 26[s] | loss 0.14\n",
      "| epoch 4 |  iter 141 / 351 | time 30[s] | loss 0.13\n",
      "| epoch 4 |  iter 161 / 351 | time 34[s] | loss 0.12\n",
      "| epoch 4 |  iter 181 / 351 | time 39[s] | loss 0.10\n",
      "| epoch 4 |  iter 201 / 351 | time 43[s] | loss 0.10\n",
      "| epoch 4 |  iter 221 / 351 | time 47[s] | loss 0.09\n",
      "| epoch 4 |  iter 241 / 351 | time 52[s] | loss 0.08\n",
      "| epoch 4 |  iter 261 / 351 | time 56[s] | loss 0.07\n",
      "| epoch 4 |  iter 281 / 351 | time 60[s] | loss 0.07\n",
      "| epoch 4 |  iter 301 / 351 | time 64[s] | loss 0.06\n",
      "| epoch 4 |  iter 321 / 351 | time 69[s] | loss 0.06\n",
      "| epoch 4 |  iter 341 / 351 | time 73[s] | loss 0.05\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 93.240%\n",
      "| epoch 5 |  iter 1 / 351 | time 0[s] | loss 0.05\n",
      "| epoch 5 |  iter 21 / 351 | time 4[s] | loss 0.04\n",
      "| epoch 5 |  iter 41 / 351 | time 8[s] | loss 0.04\n",
      "| epoch 5 |  iter 61 / 351 | time 13[s] | loss 0.03\n",
      "| epoch 5 |  iter 81 / 351 | time 17[s] | loss 0.03\n",
      "| epoch 5 |  iter 101 / 351 | time 21[s] | loss 0.03\n",
      "| epoch 5 |  iter 121 / 351 | time 26[s] | loss 0.03\n",
      "| epoch 5 |  iter 141 / 351 | time 30[s] | loss 0.02\n",
      "| epoch 5 |  iter 161 / 351 | time 34[s] | loss 0.02\n",
      "| epoch 5 |  iter 181 / 351 | time 39[s] | loss 0.02\n",
      "| epoch 5 |  iter 201 / 351 | time 43[s] | loss 0.02\n",
      "| epoch 5 |  iter 221 / 351 | time 47[s] | loss 0.01\n",
      "| epoch 5 |  iter 241 / 351 | time 51[s] | loss 0.01\n",
      "| epoch 5 |  iter 261 / 351 | time 56[s] | loss 0.01\n",
      "| epoch 5 |  iter 281 / 351 | time 60[s] | loss 0.01\n",
      "| epoch 5 |  iter 301 / 351 | time 64[s] | loss 0.01\n",
      "| epoch 5 |  iter 321 / 351 | time 69[s] | loss 0.01\n",
      "| epoch 5 |  iter 341 / 351 | time 73[s] | loss 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 99.860%\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 入力文を反転\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 5\n",
    "max_grad = 5.0\n",
    "\n",
    "# model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "z_acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    z_acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# グラフの描画\n",
    "z = np.arange(len(z_acc_list))\n",
    "plt.plot(x, acc_list, marker='o', label='attention')\n",
    "plt.plot(y, y_acc_list, marker='o', label='baseline')\n",
    "plt.plot(z, z_acc_list, marker='o', label='peeky')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "見ての通り、Attentionを用いることで学習が一段とうまくいくようになりました。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "196.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
